import re
from accelerate import Accelerator
import json
import numpy as np
def extract_data_tag(prompt):
    dialogue = prompt['text'].split("**Dialogue History**:")[1].strip()

    pattern = re.compile(r'(\S+): \((.*?)\) "(.*?)"', re.DOTALL)

    matches = pattern.findall(dialogue)

    result = [(match[0], match[1], match[2]) for match in matches]

    person = result[-1][0]  # FOX
    trait = result[-1][1]  # 괄호 안의 텍스트
    utterance = result[-1][2]  # 따옴표 안의 대화

    input_ = prompt['text'].rstrip().replace(' \"'+ utterance + '\"', '')
    return person, trait, utterance, input_

def extract_data(prompt):
    dialogue = prompt['text'].split("**Dialogue History**:")[1].strip()

    pattern = re.compile(r'(\S+):\s*(.*)')

    matches = pattern.findall(dialogue)
    
    if matches:
        person, utterance = matches[-1]
    
    input_ = prompt['text'].rstrip().replace(utterance ,'').rstrip()
    return person, utterance, input_


def read_json_file(file_path):
    """
    Reads a JSON file with multiple JSON objects (one per line) and returns the data as a list of dictionaries.
    
    Args:
        file_path (str): The path to the JSON file.
    
    Returns:
        list: A list of dictionaries containing the data from the JSON file.
    """
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                try:
                    json_obj = json.loads(line)
                    data.append(json_obj)
                except json.JSONDecodeError as e:
                    print(f"JSONDecodeError in line: {line.strip()}")
                    print(f"Error message: {e}")
        return data
    except FileNotFoundError:
        print(f"The file at {file_path} does not exist.")
        return None
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

def distinct_ngrams(sentences, n):
    """
    Calculate the distinct-n metric for a list of sentences.
    
    Args:
        sentences (list of str): The list of sentences generated by the model.
        n (int): The n-gram length.

    Returns:
        float: The distinct-n score.
    """
    ngrams = Counter()
    total_ngrams = 0

    for sentence in sentences:
        tokens = sentence.split()
        sentence_ngrams = zip(*[tokens[i:] for i in range(n)])
        ngrams.update(sentence_ngrams)
        total_ngrams += len(tokens) - n + 1
    
    return len(ngrams) / total_ngrams if total_ngrams > 0 else 0



def get_ppl(text ,model, tokenizer,device):
    encodings = tokenizer(text, return_tensors="pt")

    max_length = model.config.max_position_embeddings
    stride = 512
    seq_len = encodings.input_ids.size(1)
    
    nlls = []
    prev_end_loc = 0
    for begin_loc in tqdm(range(0, seq_len, stride)):
        end_loc = min(begin_loc + max_length, seq_len)
        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop
        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)
        target_ids = input_ids.clone()
        target_ids[:, :-trg_len] = -100
    
        with torch.no_grad():
            outputs = model(input_ids, labels=target_ids)
    
            # loss is calculated using CrossEntropyLoss which averages over valid labels
            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels
            # to the left by 1. 
            neg_log_likelihood = outputs.loss
    
        nlls.append(neg_log_likelihood)
    
        prev_end_loc = end_loc
        if end_loc == seq_len:
            break
    ppl = torch.exp(torch.stack(nlls).mean())
    return ppl    


import re
import os,sys
import torch
import evaluate
from evaluate import load
current_dir = os.getcwd()
episode_dir = os.path.abspath(os.path.join(current_dir, '..'))
sys.path.append(episode_dir)

from utils.model_utils import get_peft_checkpoint, generate,get_peft_checkpoint_
from tqdm import tqdm


from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer)

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

from collections import Counter


def evaluation_chat_system(num, prompt, model, tokenizer, device, bert_eval,rouge_eval,bleu_eval):

    
    if num == 2:
        print("This is a wo tag evaluation")
        print(prompt)
        person, utterance, input__ = extract_data(prompt)
        print(f'Last word -> {person} : "{utterance}"')
        
        input_ = tokenizer(input__, return_tensors = 'pt').to(device)     
        output = generate(model,tokenizer,
                                      input_,
                                      num_beams=1,
                                      num_return_sequences=1,
                                      max_new_tokens=100)

        print(input__)
        print("**************************")
        print(output)
        
        
        response = output.replace(input__, '')
        response = response.split("\n")[0]
        utterance = '"' + utterance + '"'
        print(f"prediction : {response}")
        print(f"Real answer : {utterance}")
            
        output_list = [response.strip()]
        last_utter_list = [utterance.strip()]
        bert_score = bert_eval.compute(predictions=output_list, references=last_utter_list, lang="en")
        rouge_score = rouge_eval.compute(predictions=output_list, references=last_utter_list)
        bleu_score = bleu_eval.compute(predictions=output_list, references=last_utter_list)
        ppl = get_ppl(output, model, tokenizer,device)
        infer = output
        print(f"Bert Score : {bert_score}")
        print(f"Rouge Score : {rouge_score}")
        print(f"bleu Score : {bleu_score}")
        print(f"ppl : {ppl}")
        return bert_score, rouge_score, bleu_score, infer, ppl
    
    
    if num == 3:
        print("This is with tag evaluation")
        print(prompt)
        person, trait, utterance, input__ = extract_data(prompt)
        print(f'Last word -> {person} : ({trait}) "{utterance}"')
        input_ = tokenizer(input__, return_tensors = 'pt').to(device)
     
        output = generate(model,tokenizer,
                                      input_,
                                      num_beams=1,
                                      num_return_sequences=1,
                                      max_new_tokens=100)
        response = output.replace(input__, '')
        response = response.split("\n")[0]
        utterance = '"' + utterance + '"'
        print(f"prediction : {response}")
        print(f"Real answer : {utterance}")
            
        output_list = [response.strip()]
        last_utter_list = [utterance.strip()]
        bert_score = bert_eval.compute(predictions=output_list, references=last_utter_list, lang="en")
        rouge_score = rouge_eval.compute(predictions=output_list, references=last_utter_list)
        bleu_score = bleu_eval.compute(predictions=output_list, references=last_utter_list)
        ppl = get_ppl(output, model, tokenizer)
        infer = output
        print(f"Bert Score : {bert_score}")
        print(f"Rouge Score : {rouge_score}")
        print(f"bleu Score : {bleu_score}")
        print(f"ppl : {ppl}")
        
        return bert_score, rouge_score, bleu_score, infer, ppl


def main():
    path = '/home/chanho/Model/SHARE/Refactorizing/result/model_save/llama wo tag'

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model, tokenizer = get_peft_checkpoint(path, device)

    path = '/home/chanho/Model/SHARE/Refactorizing/result/dataset/test_data_wo_tag.json'

    json_data = read_json_file(path)

    bertscore_eval = load("bertscore")
    rouge_eval = evaluate.load('rouge')
    bleu_eval = evaluate.load("bleu")


    bert = []
    rough = []
    bleu = []
    infer = []
    ppl_list = []


    for prompt in json_data:
        try:            
            bert_score, rough_score, bleu_score, infer_sentence, ppl = evaluation_chat_system(2, prompt ,model, tokenizer,device, bertscore_eval, rouge_eval, bleu_eval)
            bert.append(bert_score['precision'][0].cpu())
            rough.append(rough_score.cpu())
            bleu.append(bleu_score['bleu'].cpu())
            infer.append(infer_sentence)
            ppl_list.append(ppl.cpu())
        except:
            pass

    distinct_1 = distinct_ngrams(infer, 1)        
    distinct_2 = distinct_ngrams(infer, 2)
    print(f"bert score : {np.mean(bert)}, bleu score : {np.mean(bleu)}, ppl : {np.mean(ppl_list)}, disintct 1/2 : {distinct_1, distinct_2}")

if __name__ == '__main__':
    import os,sys
    os.environ["CUDA_VISIBLE_DEVICES"] = "1"
    main()

