import re
import json

def clean_text(input_text):
    """
    Removes text after the last colon, leaving the content within the last set of parentheses.
    
    Args:
        input_text (str): The input text containing the dialogue.
    
    Returns:
        str: The cleaned text.
    """
    # Find the last colon in the text
    last_colon_index = input_text.rfind(':')
    if last_colon_index == -1:
        return input_text  # No colon found, return original text
    
    # Split the text into two parts: before and after the last colon
    before_colon = input_text[:last_colon_index + 1]
    after_colon = input_text[last_colon_index + 1:]
    
    # Regular expression to match the last set of parentheses and keep it
    pattern = re.compile(r'\([^)]*\)')
    
    # Find the matching pattern
    match = pattern.search(after_colon)
    
    if match:
        # Keep the content up to and including the parentheses
        cleaned_after_colon = match.group(0)
    else:
        cleaned_after_colon = ''  # No match found, so nothing to add
    
    # Combine the cleaned part with the rest of the text
    cleaned_text = before_colon + ' ' + cleaned_after_colon
    
    return cleaned_text.strip()



def get_ppl(text ,model, tokenizer):
    encodings = tokenizer(text, return_tensors="pt")

    max_length = model.config.n_positions
    stride = 512
    seq_len = encodings.input_ids.size(1)
    
    nlls = []
    prev_end_loc = 0
    for begin_loc in tqdm(range(0, seq_len, stride)):
        end_loc = min(begin_loc + max_length, seq_len)
        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop
        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)
        target_ids = input_ids.clone()
        target_ids[:, :-trg_len] = -100
    
        with torch.no_grad():
            outputs = model(input_ids, labels=target_ids)
    
            # loss is calculated using CrossEntropyLoss which averages over valid labels
            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels
            # to the left by 1. 
            neg_log_likelihood = outputs.loss
    
        nlls.append(neg_log_likelihood)
    
        prev_end_loc = end_loc
        if end_loc == seq_len:
            break
    ppl = torch.exp(torch.stack(nlls).mean())
    return ppl    




def read_json_file(file_path):
    """
    Reads a JSON file with multiple JSON objects (one per line) and returns the data as a list of dictionaries.
    
    Args:
        file_path (str): The path to the JSON file.
    
    Returns:
        list: A list of dictionaries containing the data from the JSON file.
    """
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                try:
                    json_obj = json.loads(line)
                    data.append(json_obj)
                except json.JSONDecodeError as e:
                    print(f"JSONDecodeError in line: {line.strip()}")
                    print(f"Error message: {e}")
        return data
    except FileNotFoundError:
        print(f"The file at {file_path} does not exist.")
        return None
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

def distinct_ngrams(sentences, n):
    """
    Calculate the distinct-n metric for a list of sentences.
    
    Args:
        sentences (list of str): The list of sentences generated by the model.
        n (int): The n-gram length.

    Returns:
        float: The distinct-n score.
    """
    ngrams = Counter()
    total_ngrams = 0

    for sentence in sentences:
        tokens = sentence.split()
        sentence_ngrams = zip(*[tokens[i:] for i in range(n)])
        ngrams.update(sentence_ngrams)
        total_ngrams += len(tokens) - n + 1
    
    return len(ngrams) / total_ngrams if total_ngrams > 0 else 0


import os,sys
import torch
import evaluate
from evaluate import load
current_dir = os.getcwd()
episode_dir = os.path.abspath(os.path.join(current_dir, '..'))
sys.path.append(episode_dir)

from utils.model_utils import get_peft_checkpoint, generate
from tqdm import tqdm

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

from collections import Counter

def evaluation_chat_system(num, prompt, model, tokenizer, device):
    bertscore = load("bertscore")
    rouge = evaluate.load('rouge')
    bleu = evaluate.load("bleu")
    
    if num == 3:
        dialogue = prompt['text'].strip()
        dialogue_history = re.search(r'\*\*Dialogue History\*\*(.*)', dialogue, re.DOTALL).group(1)
        last_utter = dialogue_history.split("\n")[-1]
        
        pattern = r'\(.*?\)\s*(".*?")'
        match = re.search(pattern, last_utter).group(1)
        input = dialogue.replace(match, '')
        print(input)
        input_ = tokenizer(input, return_tensors = 'pt').to(device)
        output = generate(model,tokenizer,
                                  input_,
                                  num_beams=1,
                                  num_return_sequences=1,
                                  max_new_tokens=100)
        print('output:', output)

        output = output.replace(input, '').split("\n")[0]

        print(f"prediction : {output}")
        print(f"Real answer : {match}")
        output_list = [output.strip()]
        last_utter_list = [match.strip()]
        print(output_list)
        print(last_utter_list)
        bert_score = bertscore.compute(predictions=output_list, references=last_utter_list, lang="en")
        rouge_score = rouge.compute(predictions=output_list, references=last_utter_list)
        bleu_score = bleu.compute(predictions=output_list, references=last_utter_list)
        #ppl = get_ppl(output, model, tokenizer)
        infer = output
        #print(f"Bert Score : {bert_score}")
        #print(f"Rouge Score : {rouge_score}")
        #print(f"bleu Score : {bleu_score}")
        return bert_score, rouge_score, bleu_score, infer, 0
        

def main():
    path = '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-18-13.10.44/peft_checkpoint-5000'

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model, tokenizer = get_peft_checkpoint(path, device)

    path = '/home/chanho/Model/SHARE/Refactorizing/result/dataset/test_data.json'

    json_data = read_json_file(path)

    bert = []
    rough = []
    bleu = []
    infer = []
    ppl_list = []

    for prompt in json_data:
        bert_score, rough_score, bleu_score, infer_sentence, ppl = evaluation_chat_system(3, prompt, model, tokenizer, device)
        bert.append(bert_score)
        rough.append(rough_score)
        bleu.append(bleu_score)
        infer.append(infer_sentence)
        ppl_list.append(ppl)
    print(sum(bert)/len(bert), sum(rough)/len(rough), sum(bleu)/len(bleu), sum(infer)/len(infer), sum(ppl_list)/len(ppl_list))




if __name__ == '__main__':
    main()
