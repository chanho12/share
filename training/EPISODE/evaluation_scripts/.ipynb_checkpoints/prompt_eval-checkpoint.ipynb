{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f2d7cc-5ff6-4a64-94c9-8902bae355f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c845d7f4-b5fa-4652-b159-1ad79bfbf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# DeepSpeed Team\n",
    "import argparse\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Eval the finetued SFT model\")\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path_baseline\",\n",
    "        type=str,\n",
    "        help=\"Path to baseline model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path_finetune\",\n",
    "        type=str,\n",
    "        help=\"Path to pretrained model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_beams\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Specify num of beams',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_beam_groups\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Specify num of beams',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--top_k\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help='Specify num of beams',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--penalty_alpha\",\n",
    "        type=float,\n",
    "        default=0.6,\n",
    "        help='Specify num of beams',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_return_sequences\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Specify num of return sequences',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help='Specify num of return sequences',\n",
    "    )\n",
    "    parser.add_argument(\"--language\",\n",
    "                        type=str,\n",
    "                        default=\"English\",\n",
    "                        choices=[\"English\", \"Chinese\", \"Japanese\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             inputs,\n",
    "             num_beams=1,\n",
    "             num_beam_groups=1,\n",
    "             do_sample=False,\n",
    "             num_return_sequences=1,\n",
    "             max_new_tokens=100):\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids,\n",
    "                                  num_beams=num_beams,\n",
    "                                  num_beam_groups=num_beam_groups,\n",
    "                                  do_sample=do_sample,\n",
    "                                  num_return_sequences=num_return_sequences,\n",
    "                                  max_new_tokens=max_new_tokens)\n",
    "\n",
    "    result = tokenizer.batch_decode(generate_ids,\n",
    "                                    skip_special_tokens=True,\n",
    "                                    clean_up_tokenization_spaces=False)\n",
    "    return result\n",
    "\n",
    "def print_utils(gen_output):\n",
    "    for i in range(len(gen_output)):\n",
    "        print()\n",
    "        print(gen_output[i])\n",
    "        print()\n",
    "\n",
    "\n",
    "def prompt_eval(baseline_model, baseline_tokenizer,finetuned_model,finetuned_tokenizer, device,\n",
    "                prompts):\n",
    "        prompt = prompts['prompt']\n",
    "        dialogue = prompts['dialogue']\n",
    "        \n",
    "\n",
    "        b_inputs = baseline_tokenizer(prompt + dialogue, return_tensors=\"pt\").to(device)\n",
    "        f_inputs = finetuned_tokenizer(prompt + dialogue, return_tensors=\"pt\").to(device) \n",
    "\n",
    "        print(dialogue)\n",
    "\n",
    "        print(\"----------Baseline--------------------\")\n",
    "        b_output = generate(baseline_model,\n",
    "                          baseline_tokenizer,\n",
    "                          b_inputs,\n",
    "                          num_beams=2,\n",
    "                          num_return_sequences=1,\n",
    "                          max_new_tokens=100)\n",
    "        print(b_output[0].replace(prompt, '').replace(dialogue, ''))\n",
    "        print(\"----------finetune------------------------\")\n",
    "        f_output = generate(finetuned_model,\n",
    "                                finetuned_tokenizer,\n",
    "                                f_inputs,\n",
    "                                num_beams=2,\n",
    "                                num_return_sequences=1,\n",
    "                                max_new_tokens=100)\n",
    "        print(f_output[0].replace(prompt, '').replace(dialogue, ''))\n",
    "        # Note: we use the above simplest greedy search as the baseline. Users can also use other baseline methods,\n",
    "        # such as beam search, multinomial sampling, and beam-search multinomial sampling.\n",
    "        # We provide examples as below for users to try.\n",
    "\n",
    "        # print(\"==========finetune: Multinomial sampling=========\")\n",
    "        # r_finetune_m = generate(model_fintuned, tokenizer, inputs,\n",
    "        #                         num_beams=1,\n",
    "        #                         do_sample=True,\n",
    "        #                         num_return_sequences=args.num_return_sequences,\n",
    "        #                         max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_m)\n",
    "        # print(\"==========finetune: Beam Search=========\")\n",
    "        # r_finetune_b = generate(model_fintuned, tokenizer, inputs,\n",
    "        #                         num_beams=args.num_beams,\n",
    "        #                         num_return_sequences=args.num_return_sequences,\n",
    "        #                         max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_b)\n",
    "        # print(\"==========finetune: Beam-search multinomial sampling=========\")\n",
    "        # r_finetune_s = generate(model_fintuned, tokenizer, inputs,\n",
    "        #                         num_beams=args.num_beams,\n",
    "        #                         do_sample=True,\n",
    "        #                         num_return_sequences=args.num_return_sequences,\n",
    "        #                         max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_s)\n",
    "        # print(\"==========finetune: Diverse Beam Search=========\")\n",
    "        # r_finetune_d = generate(model_fintuned, tokenizer, inputs,\n",
    "        #                         num_beams=args.num_beams,\n",
    "        #                         num_beam_groups=args.num_beam_groups,\n",
    "        #                         num_return_sequences=args.num_return_sequences,\n",
    "        #                         max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_d)\n",
    "        # print(\"==========finetune: Constrastive Search=========\")\n",
    "        # r_finetune_c = generate_constrastive_search(model_fintuned, tokenizer, inputs,\n",
    "        #                                             top_k=args.top_k,\n",
    "        #                                             penalty_alpha=args.penalty_alpha,\n",
    "        #                                             num_return_sequences=args.num_return_sequences,\n",
    "        #                                             max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_c)\n",
    "        print(\"====================prompt end=============================\")\n",
    "\n",
    "def get_model(config, model_path, tokenizer):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        from_tf=bool(\".ckpt\" in model_path),\n",
    "        config=config,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # prepare the tokenizer and model config\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.end_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_hf_model(path, device):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, fast_tokenizer=True)\n",
    "    config = AutoConfig.from_pretrained(path)\n",
    "    model = get_model(config, path, tokenizer)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def peft_model(path, device):\n",
    "\n",
    "    config = PeftConfig.from_pretrained(path)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,fast_tokenizer=True)\n",
    "\n",
    "    model= AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, path)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    path = 'google/gemma-2b' \n",
    "\n",
    "    baseline_model, baseline_tokenizer = make_hf_model(path,device)\n",
    "\n",
    "    peft_path = '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-16-00.58.46/peft_checkpoint-14000'\n",
    "\n",
    "    peft_model, peft_tokenizer = peft_model(peft_path, device)\n",
    "\n",
    "    # One observation: if the prompt ends with a space \" \", there is a high chance that\n",
    "    # the original model (without finetuning) will stuck and produce no response.\n",
    "    # Finetuned models have less such issue. Thus following prompts all end with \":\"\n",
    "    # to make it a more meaningful comparison.\n",
    "    \n",
    "    prompts = {'prompt' : '' , 'dialogue': ''}\n",
    "\n",
    "    prompt_eval(baseline_model, baseline_tokenizer,peft_model, peft_tokenizer, device,\n",
    "                prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5d9339-6992-496f-9427-6b5666217cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e31c41a2744a1b85332feb97ba85e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "path = 'google/gemma-2b' \n",
    "\n",
    "baseline_model, baseline_tokenizer = make_hf_model(path,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027869dc-9772-4418-9c13-b2c9c135daa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdc4b5d456c4730bbfbce5df6a45f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_path = '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-16-00.58.46/peft_checkpoint-14000'\n",
    "\n",
    "peft_model, peft_tokenizer = peft_model(peft_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "73ad05bf-4fb5-40da-be96-4b2da7bbadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {'prompt' : '' , 'dialogue': ''}\n",
    "prompts['prompt'] = \"\\nTask: Generate the next response in a dialogue by focusing on the contextual cues detailed within parentheses in the dialogue history. Responses should be tailored according to the type of cue provided:\\n\\n1. Memory-driven dialogues: If the cue within parentheses details specific character traits or background context, craft responses that reflect these memory-driven elements, ensuring character consistency and rich context.\\n2. Everyday language dialogues: If the cue within parentheses is labeled \\\"Everyday Language,\\\" generate responses that are based on typical day-to-day interactions, free from specific personas or detailed context.\\n\\n\"\n",
    "\n",
    "\n",
    "text ={\"text\": \"\\nTask: Generate the next response in a dialogue by focusing on the contextual cues detailed within parentheses in the dialogue history. Responses should be tailored according to the type of cue provided:\\n\\n1. Memory-driven dialogues: If the cue within parentheses details specific character traits or background context, craft responses that reflect these memory-driven elements, ensuring character consistency and rich context.\\n2. Everyday language dialogues: If the cue within parentheses is labeled \\\"Everyday Language,\\\" generate responses that are based on typical day-to-day interactions, free from specific personas or detailed context.\\n\\n**Dialogue History**:\\nBRANDT: (BRANDT deals with operations and finances in his role , BRANDT has a dismissive opinion of someone who relies on his father's money) \\\"... Senator, the failure of one operation shouldn't cause your committee to question financing everything else we're doing down there... ... I know it looks bad, and I appreciate your support. Together we'll get it done... Yeah. 'bye. Without his father's money, that asshole'd be keeping bees for a living...\\\"\\nBRANDT: (Everyday Language) \\\"What? No.\\\"\\nUPDEGRAF: (UPDEGRAF relays communications from others such as Mr. Pitt) \\\"He's called every day.\\\"\\nBRANDT: (Everyday Language) \\\"I don't need it.\\\"\\nUPDEGRAF: (UPDEGRAF is an intermediary or assistant to BRANDT) \\\"Mr. Pitt, Mr. Brandt'll have to get back to you.\\\"\\n\\n\\n\"}\n",
    "\n",
    "\n",
    "text = text['text']\n",
    "match = re.search(r\"\\*\\*Dialogue History\\*\\*:\\s*(.+)\", text, re.DOTALL)\n",
    "\n",
    "text = match.group(1).strip()\n",
    "\n",
    "prompts['dialogue'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "90aaf860-a2ab-4f69-ab62-f217d5dd15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRANDT: (BRANDT deals with operations and finances in his role , BRANDT has a dismissive opinion of someone who relies on his father's money) \"... Senator, the failure of one operation shouldn't cause your committee to question financing everything else we're doing down there... ... I know it looks bad, and I appreciate your support. Together we'll get it done... Yeah. 'bye. Without his father's money, that asshole'd be keeping bees for a living...\"\n",
      "BRANDT: (Everyday Language) \"What? No.\"\n",
      "UPDEGRAF: (UPDEGRAF relays communications from others such as Mr. Pitt) \"He's called every day.\"\n",
      "BRANDT: (Everyday Language) \"I don't need it.\"\n",
      "UPDEGRAF: (UPDEGRAF is an intermediary or assistant to BRANDT) \"Mr. Pitt, Mr. Brandt'll have to get back to you.\"\n",
      "----------Baseline--------------------\n",
      "\n",
      "BRANDT: (Everyday Language) \"I don't need it.\"\n",
      "UPDEGRAF: (Everyday Language) \"Mr. Pitt, Mr. Brandt'll have to get back to you.\"\n",
      "BRANDT: (Everyday Language) \"I don't need it.\"\n",
      "UPDEGRAF: (Everyday Language) \"Mr. Pitt, Mr. Brandt'll have to get back to you.\"\n",
      "BRANDT: (Everyday Language) \"I don't need it.\"\n",
      "----------finetune------------------------\n",
      "\n",
      "BRANDT: (Everyday Language) \"I don't need it.\"\n",
      "UPDEGRAF: (UPDEGRAF is an intermediary or assistant to BRANDT) \"Mr. Pitt, Mr. Brandt'll have to get back to you.\"\n",
      "BRANDT: (Everyday Language) \"I don't need it.\"\n",
      "UPDEGRAF: (Everyday Language) \"Mr. Pitt, Mr. Brandt'll have to get back to you.\"\n",
      "BRANDT: (Everyday Language\n",
      "====================prompt end=============================\n"
     ]
    }
   ],
   "source": [
    "prompt_eval(baseline_model, baseline_tokenizer, peft_model, peft_tokenizer, device, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6dee7-054e-444f-a699-d20f97d7f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
