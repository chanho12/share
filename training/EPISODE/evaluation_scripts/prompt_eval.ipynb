{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f2d7cc-5ff6-4a64-94c9-8902bae355f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c845d7f4-b5fa-4652-b159-1ad79bfbf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import re\n",
    "# DeepSpeed Team\n",
    "import argparse\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Eval the finetued SFT model\")\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path_baseline\",\n",
    "        type=str,\n",
    "        help=\"Path to baseline model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path_finetune\",\n",
    "        type=str,\n",
    "        help=\"Path to pretrained model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_beams\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Specify num of beams',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_beam_groups\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Specify num of beams',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--top_k\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help='Specify num of beams',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--penalty_alpha\",\n",
    "        type=float,\n",
    "        default=0.6,\n",
    "        help='Specify num of beams',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_return_sequences\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Specify num of return sequences',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help='Specify num of return sequences',\n",
    "    )\n",
    "    parser.add_argument(\"--language\",\n",
    "                        type=str,\n",
    "                        default=\"English\",\n",
    "                        choices=[\"English\", \"Chinese\", \"Japanese\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             inputs,\n",
    "             num_beams=1,\n",
    "             num_beam_groups=1,\n",
    "             do_sample=False,\n",
    "             num_return_sequences=1,\n",
    "             max_new_tokens=100):\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids,\n",
    "                                  num_beams=num_beams,\n",
    "                                  num_beam_groups=num_beam_groups,\n",
    "                                  do_sample=do_sample,\n",
    "                                  num_return_sequences=num_return_sequences,\n",
    "                                  max_new_tokens=max_new_tokens)\n",
    "\n",
    "    result = tokenizer.batch_decode(generate_ids,\n",
    "                                    skip_special_tokens=True,\n",
    "                                    clean_up_tokenization_spaces=False)\n",
    "    return result\n",
    "\n",
    "def print_utils(gen_output):\n",
    "    for i in range(len(gen_output)):\n",
    "        print()\n",
    "        print(gen_output[i])\n",
    "        print()\n",
    "\n",
    "\n",
    "def prompt_eval(baseline_model, baseline_tokenizer,finetuned_model,finetuned_tokenizer, device,\n",
    "                prompts):\n",
    "        prompt = prompts['prompt']\n",
    "        dialogue = prompts['dialogue']\n",
    "        \n",
    "\n",
    "        b_inputs = baseline_tokenizer(prompt + dialogue, return_tensors=\"pt\").to(device)\n",
    "        f_inputs = finetuned_tokenizer(prompt + dialogue, return_tensors=\"pt\").to(device) \n",
    "\n",
    "        print(dialogue)\n",
    "\n",
    "        print(\"----------Baseline--------------------\")\n",
    "        b_output = generate(baseline_model,\n",
    "                          baseline_tokenizer,\n",
    "                          b_inputs,\n",
    "                          num_beams=2,\n",
    "                          num_return_sequences=1,\n",
    "                          max_new_tokens=100)\n",
    "        print(b_output[0].replace(prompt, '').replace(dialogue, ''))\n",
    "        print(\"----------finetune------------------------\")\n",
    "        f_output = generate(finetuned_model,\n",
    "                                finetuned_tokenizer,\n",
    "                                f_inputs,\n",
    "                                num_beams=2,\n",
    "                                num_return_sequences=1,\n",
    "                                max_new_tokens=100)\n",
    "        print(f_output[0].replace(prompt, '').replace(dialogue, ''))\n",
    "        # Note: we use the above simplest greedy search as the baseline. Users can also use other baseline methods,\n",
    "        # such as beam search, multinomial sampling, and beam-search multinomial sampling.\n",
    "        # We provide examples as below for users to try.\n",
    "\n",
    "        # print(\"==========finetune: Multinomial sampling=========\")\n",
    "        # r_finetune_m = generate(model_fintuned, tokenizer, inputs,\n",
    "        #                         num_beams=1,\n",
    "        #                         do_sample=True,\n",
    "        #                         num_return_sequences=args.num_return_sequences,\n",
    "        #                         max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_m)\n",
    "        # print(\"==========finetune: Beam Search=========\")\n",
    "        # r_finetune_b = generate(model_fintuned, tokenizer, inputs,\n",
    "        #                         num_beams=args.num_beams,\n",
    "        #                         num_return_sequences=args.num_return_sequences,\n",
    "        #                         max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_b)\n",
    "        # print(\"==========finetune: Beam-search multinomial sampling=========\")\n",
    "        # r_finetune_s = generate(model_fintuned, tokenizer, inputs,\n",
    "        #                         num_beams=args.num_beams,\n",
    "        #                         do_sample=True,\n",
    "        #                         num_return_sequences=args.num_return_sequences,\n",
    "        #                         max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_s)\n",
    "        # print(\"==========finetune: Diverse Beam Search=========\")\n",
    "        # r_finetune_d = generate(model_fintuned, tokenizer, inputs,\n",
    "        #                         num_beams=args.num_beams,\n",
    "        #                         num_beam_groups=args.num_beam_groups,\n",
    "        #                         num_return_sequences=args.num_return_sequences,\n",
    "        #                         max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_d)\n",
    "        # print(\"==========finetune: Constrastive Search=========\")\n",
    "        # r_finetune_c = generate_constrastive_search(model_fintuned, tokenizer, inputs,\n",
    "        #                                             top_k=args.top_k,\n",
    "        #                                             penalty_alpha=args.penalty_alpha,\n",
    "        #                                             num_return_sequences=args.num_return_sequences,\n",
    "        #                                             max_new_tokens=args.max_new_tokens)\n",
    "        # print_utils(r_finetune_c)\n",
    "        print(\"====================prompt end=============================\")\n",
    "\n",
    "def get_model(config, model_path, tokenizer):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        from_tf=bool(\".ckpt\" in model_path),\n",
    "        config=config,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # prepare the tokenizer and model config\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.end_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_hf_model(path, device):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, fast_tokenizer=True)\n",
    "    config = AutoConfig.from_pretrained(path)\n",
    "    model = get_model(config, path, tokenizer)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def peft_model(path, device):\n",
    "\n",
    "    config = PeftConfig.from_pretrained(path)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,fast_tokenizer=True)\n",
    "\n",
    "    model= AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, path)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def multi_gpu_peft_model(path, device):\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    config = PeftConfig.from_pretrained(path)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,fast_tokenizer=True)\n",
    "\n",
    "    model= AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, path)\n",
    "\n",
    "    model = accelerator.prepare(model)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, accelerator\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    path = 'google/gemma-2b' \n",
    "\n",
    "    baseline_model, baseline_tokenizer = make_hf_model(path,device)\n",
    "\n",
    "    peft_path = '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-16-00.58.46/peft_checkpoint-14000'\n",
    "\n",
    "    peft_model, peft_tokenizer = peft_model(peft_path, device)\n",
    "\n",
    "    # One observation: if the prompt ends with a space \" \", there is a high chance that\n",
    "    # the original model (without finetuning) will stuck and produce no response.\n",
    "    # Finetuned models have less such issue. Thus following prompts all end with \":\"\n",
    "    # to make it a more meaningful comparison.\n",
    "    \n",
    "    prompts = {'prompt' : '' , 'dialogue': ''}\n",
    "\n",
    "    prompt_eval(baseline_model, baseline_tokenizer,peft_model, peft_tokenizer, device,\n",
    "                prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5d9339-6992-496f-9427-6b5666217cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3064766aa00b40578abfb0abf6b705b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "path = 'google/gemma-2b' \n",
    "\n",
    "baseline_model, baseline_tokenizer = make_hf_model(path,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655b083e-e11a-4ebc-b9ba-c8b4f15a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "def multi_gpu_peft_model(path):\n",
    "    # Accelerator 설정\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # PEFT 모델 설정 로드\n",
    "    config = PeftConfig.from_pretrained(path)\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, fast_tokenizer=True)\n",
    "    \n",
    "    # 8-bit 모드에서 모델 로드\n",
    "    model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, \n",
    "                                                 load_ \n",
    "                                                 device_map=\"auto\")\n",
    "    \n",
    "    # PEFT 적용\n",
    "    model = PeftModel.from_pretrained(model, path)\n",
    "    \n",
    "    # 모델을 Accelerator로 준비\n",
    "    model = accelerator.prepare(model)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206bb16c-1482-4f9f-ab8e-8a22519d6bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c9f1f4fa3a4db4a4978b60825789a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_path = '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-17-21.09.49/peft_checkpoint-6000'\n",
    "\n",
    "\n",
    "peft_model, peft_tokenizer, accelerate = multi_gpu_peft_model(peft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027869dc-9772-4418-9c13-b2c9c135daa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c172bb1928d5431f9200739a412f4cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m peft_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-17-21.09.49/peft_checkpoint-6000\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m peft_model, peft_tokenizer \u001b[38;5;241m=\u001b[39m peft_model(peft_path, device)\n",
      "Cell \u001b[0;32mIn[1], line 213\u001b[0m, in \u001b[0;36mpeft_model\u001b[0;34m(path, device)\u001b[0m\n\u001b[1;32m    209\u001b[0m model\u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n\u001b[1;32m    211\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, path)\n\u001b[0;32m--> 213\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    214\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1160\u001b[0m         device,\n\u001b[1;32m   1161\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1162\u001b[0m         non_blocking,\n\u001b[1;32m   1163\u001b[0m     )\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "peft_path = '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-17-21.09.49/peft_checkpoint-6000'\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "peft_model, peft_tokenizer = peft_model(peft_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ad05bf-4fb5-40da-be96-4b2da7bbadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {'prompt' : '' , 'dialogue': ''}\n",
    "prompts['prompt'] = \"\\nTask: Generate the next response in a dialogue by focusing on the contextual cues detailed within parentheses in the dialogue history. Responses should be tailored according to the type of cue provided:\\n\\n1. Memory-driven dialogues: If the cue within parentheses details specific character traits or background context, craft responses that reflect these memory-driven elements, ensuring character consistency and rich context.\\n2. Everyday language dialogues: If the cue within parentheses is labeled \\\"Everyday Language,\\\" generate responses that are based on typical day-to-day interactions, free from specific personas or detailed context.\\n\\n\"\n",
    "\n",
    "\n",
    "text ={\"text\": \"\\nTask: Generate the next response in a dialogue by focusing on the contextual cues detailed within parentheses in the dialogue history. Responses should be tailored according to the type of cue provided:\\n\\n1. Memory-driven dialogues: If the cue within parentheses details specific character traits or background context, craft responses that reflect these memory-driven elements, ensuring character consistency and rich context.\\n2. Everyday language dialogues: If the cue within parentheses is labeled \\\"Everyday Language,\\\" generate responses that are based on typical day-to-day interactions, free from specific personas or detailed context.\\n\\n**Dialogue History**:\\nBILL: (BILL is identified as Dr. Capa) \\\"Mrs. Niedelmeyer! It's Dr. Capa again.\\\"\\nEDITH: (EDITH is identified as Mrs. Niedelmeyer , EDITH demonstrates fear and hostility towards BILL during the conversation) \\\"You leave me alone! Get away from here!\\\"\\nBILL: (Dr. Capa suggests a professional, possibly medical, occupation) \\\"I need your help, please!\\\"\\nEDITH: (EDITH demonstrates fear and hostility towards BILL during the conversation) \\\"Why are you torturing me like this?\\\"\\nBILL: (BILL is identified as Dr. Capa) \\\"Mrs. Niedelmeyer, please!\\\"\\n\\n\\n\"}\n",
    "\n",
    "\n",
    "text = text['text']\n",
    "match = re.search(r\"\\*\\*Dialogue History\\*\\*:\\s*(.+)\", text, re.DOTALL)\n",
    "\n",
    "text = match.group(1).strip()\n",
    "\n",
    "prompts['dialogue'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90aaf860-a2ab-4f69-ab62-f217d5dd15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BILL: (BILL is identified as Dr. Capa) \"Mrs. Niedelmeyer! It's Dr. Capa again.\"\n",
      "EDITH: (EDITH is identified as Mrs. Niedelmeyer , EDITH demonstrates fear and hostility towards BILL during the conversation) \"You leave me alone! Get away from here!\"\n",
      "BILL: (Dr. Capa suggests a professional, possibly medical, occupation) \"I need your help, please!\"\n",
      "EDITH: (EDITH demonstrates fear and hostility towards BILL during the conversation) \"Why are you torturing me like this?\"\n",
      "BILL: (BILL is identified as Dr. Capa) \"Mrs. Niedelmeyer, please!\"\n",
      "----------Baseline--------------------\n",
      "\n",
      "EDITH: (EDITH is identified as Mrs. Niedelmeyer , EDITH demonstrates fear and hostility towards BILL during the conversation) \"I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I\n",
      "----------finetune------------------------\n",
      "\n",
      "EDITH: (Everyday Language) \"I'm not afraid of you!\"\n",
      "BILL: (BILL is identified as Dr. Capa) \"Mrs. Niedelmeyer, please!\"\n",
      "EDITH: (Everyday Language) \"I'm not afraid of you!\"\n",
      "BILL: (BILL is identified as Dr. Capa) \"Mrs. Niedelmeyer, please!\"\n",
      "EDITH: (Everyday Language) \"I'\n",
      "====================prompt end=============================\n"
     ]
    }
   ],
   "source": [
    "prompt_eval(baseline_model, baseline_tokenizer, peft_model, peft_tokenizer, device, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6dee7-054e-444f-a699-d20f97d7f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
