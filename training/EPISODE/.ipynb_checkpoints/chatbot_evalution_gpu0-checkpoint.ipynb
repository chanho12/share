{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cdda42-096c-48c3-b264-407ec7a3c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file with multiple JSON objects (one per line) and returns the data as a list of dictionaries.\n",
    "    \n",
    "    Args:||\n",
    "        file_path (str): The path to the JSON file.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the data from the JSON file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    json_obj = json.loads(line)\n",
    "                    data.append(json_obj)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSONDecodeError in line: {line.strip()}\")\n",
    "                    print(f\"Error message: {e}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file at {file_path} does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ffa6f7-c5ae-462f-898a-55163009ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_without_tag(prompt):\n",
    "    prompts = prompt['prompt']\n",
    "\n",
    "    last_utter = f\"{prompt['last_speaker']} :\" \n",
    "\n",
    "    dialogue = prompts + last_utter\n",
    "    \n",
    "    return dialogue, prompt['last_speaker'], prompt['answer']\n",
    "    \n",
    "\n",
    "def extract_data_with_tag(prompt):\n",
    "    prompts = prompt['prompt']\n",
    "\n",
    "    last_utter = f\"{prompt['last_speaker']} : ({prompt['gold_tag']})\" \n",
    "\n",
    "    dialogue = prompts + last_utter\n",
    "    \n",
    "    return dialogue, prompt['last_speaker'], prompt['gold_tag'], prompt['answer']\n",
    "\n",
    "def distinct_ngrams(sentences, n):\n",
    "    \"\"\"\n",
    "    Calculate the distinct-n metric for a list of sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): The list of sentences generated by the model.\n",
    "        n (int): The n-gram length.\n",
    "\n",
    "    Returns:\n",
    "        float: The distinct-n score.\n",
    "    \"\"\"\n",
    "    ngrams = Counter()\n",
    "    total_ngrams = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        sentence_ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "        ngrams.update(sentence_ngrams)\n",
    "        total_ngrams += len(tokens) - n + 1\n",
    "    \n",
    "    return len(ngrams) / total_ngrams if total_ngrams > 0 else 0\n",
    "\n",
    "\n",
    "def get_ppl(text ,model, tokenizer):\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "    \n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1. \n",
    "            neg_log_likelihood = outputs.loss\n",
    "    \n",
    "        nlls.append(neg_log_likelihood)\n",
    "    \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1787f2-0d79-49be-9de8-ce82fdb060eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import torch\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "current_dir = os.getcwd()\n",
    "episode_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(episode_dir)\n",
    "\n",
    "from utils.model_utils import get_peft_checkpoint, generate, get_peft_checkpoint_\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer)\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def evaluation_chat_system(num, prompt , model, tokenizer, device, bert_eval,rough_eval):\n",
    "    \n",
    "    if num == 2:\n",
    "        print(\"This is a wo tag evaluation\")\n",
    "        print(prompt)\n",
    "        input__, person, utterance,  = extract_data_without_tag(prompt)\n",
    "        print(f'Last word -> {person} : \"{utterance}\"')\n",
    "        \n",
    "        input_ = tokenizer(input__, return_tensors = 'pt').to(device)     \n",
    "        output = generate(model,tokenizer,\n",
    "                                      input_,\n",
    "                                      num_beams=1,\n",
    "                                      num_return_sequences=1,\n",
    "                                      max_new_tokens=100)\n",
    "\n",
    "\n",
    "        \n",
    "        response = output.replace(input__, '')\n",
    "        response = response.split(\"\\n\")[0]\n",
    "        print(f\"prediction : {response}\")\n",
    "        print(f\"Real answer : {utterance}\")\n",
    "\n",
    "        reference = [utterance.split()]\n",
    "        candidate = response.split()\n",
    "\n",
    "            \n",
    "        output_list = [response.strip()]\n",
    "        last_utter_list = [utterance.strip()]\n",
    "        #evalation\n",
    "        bert_score = bert_eval.compute(predictions=output_list, references=last_utter_list, lang=\"en\")\n",
    "        rouge_score = rouge_eval.compute(predictions=output_list, references=last_utter_list)\n",
    "\n",
    "        ## bleu\n",
    "\n",
    "        weights_unigram = (1, 0, 0, 0)\n",
    "        bleu_unigram = sentence_bleu(reference, candidate, weights=weights_unigram, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "        weights_bigram = (0.5, 0.5, 0, 0)\n",
    "        bleu_bigram = sentence_bleu(reference, candidate, weights=weights_bigram, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "        ### ppl\n",
    "        ppl = get_ppl(response, model, tokenizer)\n",
    "        \n",
    "        print(f\"Bert Score : {bert_score}\")\n",
    "        print(f\"Rouge Score : {rouge_score}\")\n",
    "        print(f\"bleu 1/2 : {bleu_unigram} {bleu_bigram}\")\n",
    "        print(f\"ppl : {ppl}\")\n",
    "        return bert_score, rouge_score, bleu_unigram, bleu_bigram, response ,ppl\n",
    "    \n",
    "    \n",
    "    if num == 3:\n",
    "        print(\"This is with tag evaluation\")\n",
    "        print(prompt)\n",
    "        input__ , person, trait, utterance = extract_data_with_tag(prompt)\n",
    "        print(f'Last word -> {person} : ({trait}) \"{utterance}\"')\n",
    "        \n",
    "        input_ = tokenizer(input__, return_tensors = 'pt').to(device)\n",
    "     \n",
    "        output = generate(model,tokenizer,\n",
    "                                      input_,\n",
    "                                      num_beams=1,\n",
    "                                      num_return_sequences=1,\n",
    "                                      max_new_tokens=100)\n",
    "\n",
    "\n",
    "        ### utterance : correct answer\n",
    "        ### response : Model-generated answer\n",
    "\n",
    "        response = output.replace(input__, '').split(\"\\n\")[0]\n",
    "        \n",
    "        print(f\"prediction : {response}\")\n",
    "        print(f\"Real answer : {utterance}\")\n",
    "        \n",
    "        output_list = [response.strip()]\n",
    "        last_utter_list = [utterance.strip()]\n",
    "\n",
    "        reference = [utterance.split()]\n",
    "        candidate = response.split()\n",
    "\n",
    "        #evalation\n",
    "        bert_score = bert_eval.compute(predictions=output_list, references=last_utter_list, lang=\"en\")\n",
    "        rouge_score = rouge_eval.compute(predictions=output_list, references=last_utter_list)\n",
    "\n",
    "        ## bleu\n",
    "\n",
    "        weights_unigram = (1, 0, 0, 0)\n",
    "        bleu_unigram = sentence_bleu(reference, candidate, weights=weights_unigram, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "        weights_bigram = (0.5, 0.5, 0, 0)\n",
    "        bleu_bigram = sentence_bleu(reference, candidate, weights=weights_bigram, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "        ### ppl\n",
    "        ppl = get_ppl(response, model, tokenizer)\n",
    "        \n",
    "        print(f\"Bert Score : {bert_score}\")\n",
    "        print(f\"Rouge Score : {rouge_score}\")\n",
    "        print(f\"bleu 1/2 : {bleu_unigram} {bleu_bigram}\")\n",
    "        print(f\"ppl : {ppl}\")\n",
    "\n",
    "        \n",
    "        return bert_score, rouge_score, bleu_unigram, bleu_bigram, response ,ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b6e550-1516-4860-8d0c-d2d280e7cc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf36739adb24970b8066a95f82ce17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'chano12/gemma_without_tag'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, tokenizer = get_peft_checkpoint_(path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd21ab89-9aea-4907-a847-62cfb090e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = '/home/chanho/Model/SHARE/Refactorizing/result/dataset/test_without_tag.json'\n",
    "\n",
    "json_data = read_json_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a05f1c-a76e-423f-b3c4-3ad53bea4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = []\n",
    "rough = []\n",
    "bleu_1_list = []\n",
    "bleu_2_list = []\n",
    "infer = []\n",
    "ppl_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5acf90aa-20e6-4b88-b067-b744aaffb3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with tag evaluation\n",
      "{'prompt': \"\\nTask: Generate the next response in the dialogue based on the provided history. The response should logically follow and predict the next reply considering the context of the conversation.\\n\\n**Dialogue History**:\\nLOLA: Hello, Mr. Neff. It's me.\\nNEFF: Something the matter?\\nLOLA: I've been waiting for you.\\nNEFF: For me? What for?\\nLOLA: I thought you could let me ride with you, if you're going my way.\\n\\n\", 'answer': \"Which way would that be? Oh, sure. Vermont and Franklin. North- west corner, wasn't it? Be glad to, Miss Dietrichson.\", 'gold_tag': 'NEFF is familiar with the local geographic area , NEFF references specific streets', 'last_speaker': 'NEFF'}\n",
      "Last word -> NEFF : (NEFF is familiar with the local geographic area , NEFF references specific streets) \"Which way would that be? Oh, sure. Vermont and Franklin. North- west corner, wasn't it? Be glad to, Miss Dietrichson.\"\n",
      "prediction :  Why don't you ride with me on the way to dinner? Then you can go wherever you want, because I'll\n",
      "Real answer : Which way would that be? Oh, sure. Vermont and Franklin. North- west corner, wasn't it? Be glad to, Miss Dietrichson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6938e567de2244379ebb70bbc929ffa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "chano12/gemma_without_tag does not appear to have a file named config.json. Checkout 'https://huggingface.co/chano12/gemma_without_tag/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/chano12/gemma_without_tag/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    399\u001b[0m         path_or_repo_id,\n\u001b[1;32m    400\u001b[0m         filename,\n\u001b[1;32m    401\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    402\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    403\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    404\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    405\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    406\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    407\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    408\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    409\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    410\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1222\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1224\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1226\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1227\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1228\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1231\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1232\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1233\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1236\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1237\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1282\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m _get_metadata_or_catch_error(\n\u001b[1;32m   1283\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1284\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1285\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1286\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1287\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1288\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1289\u001b[0m     etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1290\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1291\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1292\u001b[0m     storage_folder\u001b[38;5;241m=\u001b[39mstorage_folder,\n\u001b[1;32m   1293\u001b[0m     relative_filename\u001b[38;5;241m=\u001b[39mrelative_filename,\n\u001b[1;32m   1294\u001b[0m )\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1646\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1647\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1648\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1649\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1650\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1651\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1652\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1653\u001b[0m )\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m    373\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    374\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    375\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-665bc975-124bdebd692d1d4c75f94c0a;eed1a1db-4ef8-4189-8e4f-23f624209c99)\n\nEntry Not Found for url: https://huggingface.co/chano12/gemma_without_tag/resolve/main/config.json.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m json_data:\n\u001b[0;32m----> 6\u001b[0m     bert_score, rough_score, bleu_1, bleu_2, infer_sentence, ppl \u001b[38;5;241m=\u001b[39m evaluation_chat_system(\u001b[38;5;241m3\u001b[39m, prompt ,model, tokenizer,device, bertscore_eval, rouge_eval,perplexity)\n\u001b[1;32m      7\u001b[0m     bert\u001b[38;5;241m.\u001b[39mappend(bert_score)\n\u001b[1;32m      8\u001b[0m     rough\u001b[38;5;241m.\u001b[39mappend(rough_score)\n",
      "Cell \u001b[0;32mIn[12], line 113\u001b[0m, in \u001b[0;36mevaluation_chat_system\u001b[0;34m(num, prompt, model, tokenizer, device, bert_eval, rough_eval, perplexity)\u001b[0m\n\u001b[1;32m    110\u001b[0m bleu_bigram \u001b[38;5;241m=\u001b[39m sentence_bleu(reference, candidate, weights\u001b[38;5;241m=\u001b[39mweights_bigram, smoothing_function\u001b[38;5;241m=\u001b[39mSmoothingFunction()\u001b[38;5;241m.\u001b[39mmethod1)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m### ppl\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m ppl \u001b[38;5;241m=\u001b[39m perplexity\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39moutput_list, model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchano12/gemma_without_tag\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBert Score : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbert_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRouge Score : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrouge_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/evaluate/module.py:467\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {input_name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed):\n\u001b[0;32m--> 467\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompute_kwargs)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--perplexity/8ab643ad86f568b7d1d5f7822373fa7401ff5ff0297ccf114b0ca6a33be96bc0/perplexity.py:117\u001b[0m, in \u001b[0;36mPerplexity._compute\u001b[0;34m(self, predictions, model_id, batch_size, add_start_token, device, max_length)\u001b[0m\n\u001b[1;32m    114\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m    115\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 117\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# if batch_size > 1 (which generally leads to padding being required), and\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# if there is not an already assigned pad_token, assign an existing\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# special token to also be the padding token\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:820\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 820\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    821\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    822\u001b[0m         )\n\u001b[1;32m    823\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:931\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    932\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    687\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    688\u001b[0m         configuration_file,\n\u001b[1;32m    689\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    690\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    691\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    692\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    693\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    694\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    695\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    696\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    697\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    698\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    699\u001b[0m     )\n\u001b[1;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/utils/hub.py:452\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m         revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    457\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001b[0;31mOSError\u001b[0m: chano12/gemma_without_tag does not appear to have a file named config.json. Checkout 'https://huggingface.co/chano12/gemma_without_tag/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "bertscore_eval = load(\"bertscore\")\n",
    "rouge_eval = evaluate.load('rouge') \n",
    "\n",
    "for prompt in json_data:\n",
    "    bert_score, rough_score, bleu_1, bleu_2, infer_sentence, ppl = evaluation_chat_system(3, prompt ,model, tokenizer,device, bertscore_eval, rouge_eval)\n",
    "    bert.append(bert_score)\n",
    "    rough.append(rough_score)\n",
    "    bleu_1_list.append(bleu_1)\n",
    "    bleu_2_list.append(bleu_2)\n",
    "    infer.append(infer_sentence)\n",
    "    ppl_list.append(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb19f8-f35c-4139-a86b-5052afefb925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ber = [i['precision'][0] for i in bert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ade18-3cbf-45ed-873b-66134a5caf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "818fb296-add7-4a7f-95aa-650551c84d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(tensor_list):\n",
    "    # Move tensors to CPU and convert to numpy arrays\n",
    "    valid_tensors = [t for t in tensor_list if not torch.isnan(t)]\n",
    "    cpu_tensors = [t.cpu().numpy() for t in valid_tensors]\n",
    "    cpu_tensors = [float(i) for i in cpu_tensors]\n",
    "\n",
    "    return cpu_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15247f07-d59e-43fa-9e9c-d55a8b200c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722.6492043087733"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sorted(calculate_mean(ppl_list))[:-50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b611d7-82c2-4939-a5ca-ee0328e774ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL : 1351.1161067331393, \n",
      "BertScore : 0.8786377304240572 \n",
      "rouge1 : 0.19238813223084816 \n",
      "rouge2 : 0.07725592091197174 \n",
      "rougeL : 0.17753343392079152 \n",
      "rougeLsum : 0.17753343392079152, \n",
      "bleu_1 : 0.0851429316663488 \n",
      "bleu_2 : 0.053337645109041605 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ber = [i['precision'][0] for i in bert]\n",
    "\n",
    "rouge1 = np.mean([i['rouge1'] for i in rough])\n",
    "\n",
    "rouge2 = np.mean([i['rouge2'] for i in rough])\n",
    "\n",
    "rougeL = np.mean([i['rougeL'] for i in rough])\n",
    "\n",
    "rougeLsum = np.mean([i['rougeLsum'] for i in rough])\n",
    "\n",
    "mean_bleu_1 = np.mean(bleu_1_list)\n",
    "\n",
    "mean_bleu_2 = np.mean(bleu_2_list)\n",
    "\n",
    "print(f'PPL : {np.mean(calculate_mean(ppl_list))}, \\nBertScore : {np.mean(ber)} \\nrouge1 : {rouge1} \\nrouge2 : {rouge2} \\nrougeL : {rougeL} \\nrougeLsum : {rougeLsum}, \\nbleu_1 : {mean_bleu_1} \\nbleu_2 : {mean_bleu_2} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47caa1ae-711e-4060-9a71-15f64f51090e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21012395160691755"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_ngrams(infer, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08fa72dc-9ad3-467a-b3ec-952aee300ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5886313277654679"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "distinct_ngrams(infer, 1)\n",
    "distinct_ngrams(infer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caa08015-d452-4e88-9ef5-7c826b6bc1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8492068219184875"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd9710af-4079-465a-8c69-380b3710f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge1 = np.mean([i['rouge1'] for i in rough])\n",
    "\n",
    "rouge2 = np.mean([i['rouge2'] for i in rough])\n",
    "\n",
    "rougeL = np.mean([i['rougeL'] for i in rough])\n",
    "\n",
    "rougeLsum = np.mean([i['rougeLsum'] for i in rough])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6046ebd4-0ad1-49e2-b439-f32c6c1b1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bleu = np.mean([i['bleu'] for i in bleu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fa16661-0c26-4fc1-97e5-857f1b8d11ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL : 6.603209018707275, \n",
      "BertScore : 0.8492068219184875 \n",
      "rouge1 : 0.0864367417136198 \n",
      "rouge2 : 0.016000117526532315 \n",
      "rougeL : 0.07990215447014093 \n",
      "rougeLsum : 0.07990215447014093, \n",
      "bleu : 0.007389715194612174\n"
     ]
    }
   ],
   "source": [
    "print(f'PPL : {calculate_mean(ppl_list)}, \\nBertScore : {np.mean(ber)} \\nrouge1 : {rouge1} \\nrouge2 : {rouge2} \\nrougeL : {rougeL} \\nrougeLsum : {rougeLsum}, \\nbleu : {mean_bleu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230233e6-40f8-416c-ac0e-ba6b52dfbfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
