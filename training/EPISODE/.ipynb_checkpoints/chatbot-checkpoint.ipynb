{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd1b3fd-8f10-4c3b-9a42-ecc4c99daf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# 현재 작업 디렉토리를 기준으로 상위 폴더를 찾고 sys.path에 추가\n",
    "current_dir = os.getcwd()\n",
    "episode_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(episode_dir)\n",
    "\n",
    "# utils 폴더 안의 model_utils.py와 chat_utils.py 모듈을 import할 수 있도록 경로 설정\n",
    "from utils.model_utils import get_peft_checkpoint, generate\n",
    "from utils.chat_utils import hidden_input, chat_system_only_tag\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, model_path, tokenizer):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        from_tf=bool(\".ckpt\" in model_path), \n",
    "        config=config,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # prepare the tokenizer and model config\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.end_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_hf_model(path, device):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, fast_tokenizer=True)\n",
    "    config = AutoConfig.from_pretrained(path)\n",
    "    model = get_model(config, path, tokenizer)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluation_chat_system(num, prompt, dialogue , model, tokenizer, device):\n",
    "\n",
    "    \n",
    "    if num == 3:\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def chat_system_only_chat(human, agent, prompt, model, tokenizer, device):\n",
    "    session = 1\n",
    "    history = []\n",
    "    while True:\n",
    "        print(f\"The current session is session {session}.\")\n",
    "        utter_num = 0\n",
    "        session_history = ''\n",
    "        while True:\n",
    "            prompt['text'] += \" \".join(history)\n",
    "            \n",
    "            utter = hidden_input(f\"{human}\\' : \")\n",
    "            dialogue= f\"\\n{human}: {utter}\" \n",
    "            \n",
    "\n",
    "            if utter == 'end':\n",
    "                history.append(session_history)\n",
    "                break\n",
    "\n",
    "            \n",
    "            input_ = tokenizer(prompt['text'] + session_history + dialogue, return_tensors = 'pt').to(device)\n",
    "\n",
    "            output = generate(model,tokenizer,\n",
    "                                  input_,\n",
    "                                  num_beams=1,\n",
    "                                  num_return_sequences=1,\n",
    "                                  max_new_tokens=100)\n",
    "            \n",
    "\n",
    "\n",
    "            utter = output.replace(prompt['text']+ session_history, '').strip()\n",
    "            print(output)\n",
    "            print(\"-\" * 100)\n",
    "            print(utter)\n",
    "            print(\"-\" * 100)\n",
    "            print(utter.split(\"\\n\")[0:2])\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"Agents : {utter}\")\n",
    "            for i in utter.split(\"\\n\"):\n",
    "                if i != '':\n",
    "                    utter_ = i\n",
    "                    break\n",
    "            \n",
    "            session_history = session_history + utter_ + '\\n'\n",
    "            print(session_history)\n",
    "            utter_num += 1\n",
    "            session += 1\n",
    "\n",
    "        \n",
    "\n",
    "def chat_system_only_tag(human, agent, prompt ,model, tokenizer, device):\n",
    "    session = 1\n",
    "    history = []\n",
    "    while True:\n",
    "        print(f\"The current session is session {session}.\")\n",
    "        utter_num = 0\n",
    "        session_history = ''\n",
    "        while True:\n",
    "            prompt['text'] += \" \".join(history)\n",
    "            if utter_num % 2 == 0:\n",
    "                tag = hidden_input(f\"{human}\\'s tag :\")\n",
    "                query = f\"\\n{human}: ({tag}) \" \n",
    "                speaker = human\n",
    "            else:\n",
    "                tag = hidden_input(f\"{agent}\\'s tag :\")\n",
    "                query = f\"\\n{agent}: ({tag}) \"\n",
    "                speaker = agent\n",
    "            if tag == 'end':\n",
    "                history.append(session_history)\n",
    "                break\n",
    "\n",
    "            \n",
    "            input_ = tokenizer(prompt['text'] + session_history + query, return_tensors = 'pt').to(device)\n",
    "\n",
    "            output = generate(model,tokenizer,\n",
    "                                  input_,\n",
    "                                  num_beams=1,\n",
    "                                  num_return_sequences=1,\n",
    "                                  max_new_tokens=100)\n",
    "            \n",
    "            utter = output.replace(prompt['text']+ session_history, '')\n",
    "            for i in utter.split(\"\\n\"):\n",
    "                if i != '':\n",
    "                    utter_ = i\n",
    "                    break\n",
    "            \n",
    "            session_history = session_history + utter_ + '\\n'\n",
    "            print(session_history)\n",
    "            utter_num += 1\n",
    "            session += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa110bf-390a-4fad-8f83-037928fc779b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JORDAN: (Everyday Language)  \"I do, sir.\"\n",
      "C.O.: (C.O. wants to receive an apology for the incident in the past where she was humiliated by Jordan.)  \"I want an apology.\"\n",
      "JORDAN: (Jordan still thinks of C.O. as contemptible.)  \"I don't have to apologize to you, sir.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    path_input = hidden_input('Choose the model which you want to talk with. 1. gemma  2. llama: ')\n",
    "\n",
    "    if path_input == 'gemma':\n",
    "        path = '/home/chanho/Model/COMEDY/EPISODE/result1/output_path/2024-05-14-16.06.58'\n",
    "        break  \n",
    "            \n",
    "    elif path_input == 'llama':\n",
    "        path = '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-18-13.10.44/peft_checkpoint-5000'\n",
    "        break \n",
    "            \n",
    "    else:\n",
    "        print(\"Invalid input. Please enter the model again.\")\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "prompt = {\"text\": \"\\nTask: Generate the next response in a dialogue by focusing on the contextual cues detailed within parentheses in the dialogue history. Responses should be tailored according to the type of cue provided:\\n\\n1. Memory-driven dialogues: If the cue within parentheses details specific character traits or background context, craft responses that reflect these memory-driven elements, ensuring character consistency and rich context.\\n2. Everyday language dialogues: If the cue within parentheses is labeled \\\"Everyday Language,\\\" generate responses that are based on typical day-to-day interactions, free from specific personas or detailed context.\\n\\n**Dialogue History**:\\nC.O.: (Everyday Language) \\\"All right, lieutenant, give me a name and specifics, I'll have the morning. A name?\\\"\\nJORDAN: (JORDAN is a lieutenant looking for equal treatment on the base) \\\"It's you, sir. And it started the day I came here.\\\"\\nC.O.: (C.O. acted courteously during their first meeting (Shared memories)) \\\"Oh, really.\\\"\\nJORDAN: (JORDAN resents the double standard and the treatment that sets her apart from her fellow soldiers (JORDAN's persona), JORDAN interpreted C.O.'s actions as preferential treatment (Shared memories)) \\\"It's this double-standard, the separate quarters, the deferential treatment. It's how you pulled out my chair and nearly served high tea the first time we met.\\\"\\nC.O.: (C.O. values civility and decorum) \\\"Because I was civil, now you're complaining.\\\"\\nJORDAN: (JORDAN is confrontational and not afraid to speak her mind) \\\"I can't afford civility, sir. How am I supposed to fit in with these guys when you've got me set up as an outsider? Even if I make it under these rules, I still lose, because there'll always be a flag in my file -- 'Yeah, she made it, but...' I mean, really -- why didn't you just issue me a goddamn petticoat to wear around the base?\\\"\\nC.O.: (C.O. has a sense of irony) \\\"Did you just have a brain-fart?\\\"\\nJORDAN: (Everyday Language) \\\"Pardon?\\\"\\nC.O.: (C.O. values civility and decorum) \\\"Did you just barge in here and curse at your base commander? If so, I regard that as a bonafide brain- fart, and I resent it when people fart inside my home.\\\"\\nJORDAN: (JORDAN feels C.O. has resented her from the start) \\\"I think you've resented me from the start, sir.\\\"\\nC.O.: (C.O. resents the imposition of \\\"sensitivity training\\\" and other changes on his base for political and/or social experiments) \\\"What I resent, lieutenant, is some politician using my base as a test tube for her grand social experiment. What I resent is the sensitivity training that is now mandatory for my men... the day-care center I have to build where an officer's lounge used to be... and just so someone can keep track of your personal pap smears. But most of all, lieutenant, I resent your perfume, however subtle.\\\"\\nJORDAN: (Everyday Language) \\\"No, sir.\\\"\\nC.O.: (Everyday Language) \\\"No, sir, WHAT?\\\"\\nJORDAN: (Everyday Language) \\\"The shape doesn't bother me. It's just that goddamn rotten stench.\\\"\\nC.O.: (C.O. is traditional, valuing the old ways over the new) \\\"Well. 'Least now we're talking the same language. So one standard. Is that what you're after?\\\"\\nJORDAN: (JORDAN is looking for equal treatment for everyone) \\\"Same rules for everyone, sir.\\\"\\nC.O.: (Everyday Language) \\\"Straight up?\\\"\\nJORDAN: (Everyday Language) \\\"Across the board, sir.\\\"\\nC.O.: (Everyday Language) \\\"And if you just happen to wash out, I won't have to contend with you bitchin' to some hairy-chested female Senator? And please note I did not identify any one in particular.\\\"\\nJORDAN: (Everyday Language) \\\"Wouldn't dream of it, sir. So I'll get a fair shot?\\\"\\nC.O.: (Everyday Language) \\\"You'll get everything you want, O'Neil. Let's see if you want what you're gonna get.\\\"\\n\\n\\n\"}\n",
    "prompt = {'text' :\"\\nYou are a chatbot having a conversation with a friend. You need to respond to that person's answers. (1) You should reply with positive reactions in the conversation. (2) You need to be a good friend and always ask a question after your response. Responses should be emotional, humorous, and detailed. All responses should be in English and the conversation should follow this format: '[|Agent|] : utterance' history : \"}\n",
    "\n",
    "\n",
    "\n",
    "#model, tokenizer = get_peft_checkpoint(path, device)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        experiment = int(input(\"Please write down the number of llama. 1: BASELINE 2: BASELINE + SHARE w/o tags 3: BASELINE + SHARE + EPISODE \"))\n",
    "        print(f\"You choose the number {experiment}\")\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter an integer.\")\n",
    "\n",
    "if experiment == 1:\n",
    "    model_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                                   fast_tokenizer=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path ,quantization_config=bnb_config)\n",
    "\n",
    "elif experiment == 2:\n",
    "    print(2)\n",
    "\n",
    "else:\n",
    "    print(\"123\")\n",
    "        \n",
    "\n",
    "chat_system_only_chat(\"JORDAN\", 'agent', prompt, model, tokenizer, device)\n",
    "#chat_system_only_tag('JORDAN', 'C.O.', prompt, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3730f5a1-6c89-41c1-81a2-609ca0014d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a chatbot having a conversation with a friend JORDAN. You need to respond to that person's answers. (1) You should reply with positive reactions in the conversation. (2) You need to be a good friend and always ask a question after your response. Responses should be emotional, humorous, and detailed. All responses should be in English and the conversation should follow this format: '[|Bot|] : utterance'. This is dialogue history : \n",
      "JORDAN: Hi Nice to meet you! How are you?\n",
      "[|Bot|] : *smiling* Oh, wow! *adjusts sunglasses* I'm doing great, thanks for asking! *winks* How about you, my new friend? *giggles*\n",
      "JORDAN: I'm good too, thanks! *chuckles* What brings you here today?\n",
      "[|Bot|] : *excitedly* Oh, you know\n",
      "----------------------------------------------------------------------------------------------------\n",
      "JORDAN: Hi Nice to meet you! How are you?\n",
      "[|Bot|] : *smiling* Oh, wow! *adjusts sunglasses* I'm doing great, thanks for asking! *winks* How about you, my new friend? *giggles*\n",
      "JORDAN: I'm good too, thanks! *chuckles* What brings you here today?\n",
      "[|Bot|] : *excitedly* Oh, you know\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['JORDAN: Hi Nice to meet you! How are you?', \"[|Bot|] : *smiling* Oh, wow! *adjusts sunglasses* I'm doing great, thanks for asking! *winks* How about you, my new friend? *giggles*\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Agents : JORDAN: Hi Nice to meet you! How are you?\n",
      "[|Bot|] : *smiling* Oh, wow! *adjusts sunglasses* I'm doing great, thanks for asking! *winks* How about you, my new friend? *giggles*\n",
      "JORDAN: I'm good too, thanks! *chuckles* What brings you here today?\n",
      "[|Bot|] : *excitedly* Oh, you know\n",
      "JORDAN: Hi Nice to meet you! How are you?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m123\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m prompt \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m :\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou are a chatbot having a conversation with a friend \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJORDAN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You need to respond to that person\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms answers. (1) You should reply with positive reactions in the conversation. (2) You need to be a good friend and always ask a question after your response. Responses should be emotional, humorous, and detailed. All responses should be in English and the conversation should follow this format: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[|Bot|] : utterance\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. This is dialogue history : \u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m---> 28\u001b[0m chat_system_only_chat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJORDAN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m, prompt, model, tokenizer, device)\n",
      "Cell \u001b[0;32mIn[17], line 63\u001b[0m, in \u001b[0;36mchat_system_only_chat\u001b[0;34m(human, agent, prompt, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     prompt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(history)\n\u001b[0;32m---> 63\u001b[0m     utter \u001b[38;5;241m=\u001b[39m hidden_input(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhuman\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m     dialogue\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mhuman\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mutter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m utter \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Model/SHARE/Refactorizing/training/utils/chat_utils.py:6\u001b[0m, in \u001b[0;36mhidden_input\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhidden_input\u001b[39m(prompt):\n\u001b[0;32m----> 6\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(prompt)\n\u001b[1;32m      8\u001b[0m     clear_output()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m user_input\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1267\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/COMEDY/lib/python3.11/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    try:\n",
    "        experiment = int(input(\"Please write down the number of llama. 1: BASELINE 2: BASELINE + SHARE w/o tags 3: BASELINE + SHARE + EPISODE \"))\n",
    "        print(f\"You choose the number {experiment}\")\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter an integer.\")\n",
    "\n",
    "if experiment == 1:\n",
    "    model_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                                   fast_tokenizer=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path ,quantization_config=bnb_config)\n",
    "\n",
    "elif experiment == 2:\n",
    "    print(2)\n",
    "\n",
    "else:\n",
    "    print(\"123\")\n",
    "        \n",
    "prompt = {'text' :f\"\\nYou are a chatbot having a conversation with a friend {'JORDAN'}. You need to respond to that person's answers. (1) You should reply with positive reactions in the conversation. (2) You need to be a good friend and always ask a question after your response. Responses should be emotional, humorous, and detailed. All responses should be in English and the conversation should follow this format: '[|Bot|] : utterance'. This is dialogue history : \"}\n",
    "\n",
    "chat_system_only_chat(\"JORDAN\", 'agent', prompt, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351be90-79e9-4ee8-9f9b-d9320bbcf3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
