[2024-05-31 14:00:06,854] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-05-31 14:00:07,433] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-31 14:00:07,433] [INFO] [runner.py:568:main] cmd = /home/chanho/anaconda3/envs/COMEDY/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=12340 --enable_each_rank_log=None training/EPISODE/main.py --model_name gemma --model_name_or_path /home/chanho/Model/SHARE/Refactorizing/result/output_path --train_data_path /home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json --valid_data_path /home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --data_output_path /home/chanho/Model/SHARE/Refactorizing/result/output_path/data --max_seq_len 2048 --learning_rate 1e-5 --weight_decay 0.1 --num_train_epochs 50 --num_train_samples 14255 /home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json --gradient_accumulation_steps 1 --lr_scheduler_type cosine --num_warmup_steps 1000 --seed 42 --zero_stage 2 --save_interval 1000 --eval_interval 100 --output_dir /home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04
[2024-05-31 14:00:08,915] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-05-31 14:00:09,574] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-05-31 14:00:09,574] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-05-31 14:00:09,574] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-05-31 14:00:09,574] [INFO] [launch.py:164:main] dist_world_size=2
[2024-05-31 14:00:09,574] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-05-31 14:00:09,574] [INFO] [launch.py:256:main] process 1412641 spawned with command: ['/home/chanho/anaconda3/envs/COMEDY/bin/python', '-u', 'training/EPISODE/main.py', '--local_rank=0', '--model_name', 'gemma', '--model_name_or_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path', '--train_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--valid_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--data_output_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', '--max_seq_len', '2048', '--learning_rate', '1e-5', '--weight_decay', '0.1', '--num_train_epochs', '50', '--num_train_samples', '14255', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '1000', '--seed', '42', '--zero_stage', '2', '--save_interval', '1000', '--eval_interval', '100', '--output_dir', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04']
[2024-05-31 14:00:09,575] [INFO] [launch.py:256:main] process 1412642 spawned with command: ['/home/chanho/anaconda3/envs/COMEDY/bin/python', '-u', 'training/EPISODE/main.py', '--local_rank=1', '--model_name', 'gemma', '--model_name_or_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path', '--train_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--valid_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--data_output_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', '--max_seq_len', '2048', '--learning_rate', '1e-5', '--weight_decay', '0.1', '--num_train_epochs', '50', '--num_train_samples', '14255', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '1000', '--seed', '42', '--zero_stage', '2', '--save_interval', '1000', '--eval_interval', '100', '--output_dir', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04']
[2024-05-31 14:00:12,247] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-31 14:00:12,283] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Namespace(data_path=['Dahoas/rm-static'], data_split='6,2,2', data_output_path='/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', model_name_or_path='/home/chanho/Model/SHARE/Refactorizing/result/output_path', model_name='gemma', per_device_train_batch_size=1, per_device_eval_batch_size=1, max_seq_len=2048, learning_rate=1e-05, weight_decay=0.1, num_train_epochs=50, gradient_accumulation_steps=1, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=1000, output_dir='/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04', seed=42, local_rank=0, gradient_checkpointing=False, offload=False, zero_stage=2, lora_dim=0, lora_module_name='decoder.layers.', only_optimize_lora=False, save_interval=1000, log_interval=10, eval_interval=100, train_data_path='/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', valid_data_path='/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', num_train_samples=14255, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None)
['/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json']
[2024-05-31 14:00:12,690] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-31 14:00:12,690] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Namespace(data_path=['Dahoas/rm-static'], data_split='6,2,2', data_output_path='/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', model_name_or_path='/home/chanho/Model/SHARE/Refactorizing/result/output_path', model_name='gemma', per_device_train_batch_size=1, per_device_eval_batch_size=1, max_seq_len=2048, learning_rate=1e-05, weight_decay=0.1, num_train_epochs=50, gradient_accumulation_steps=1, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=1000, output_dir='/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04', seed=42, local_rank=1, gradient_checkpointing=False, offload=False, zero_stage=2, lora_dim=0, lora_module_name='decoder.layers.', only_optimize_lora=False, save_interval=1000, log_interval=10, eval_interval=100, train_data_path='/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', valid_data_path='/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', num_train_samples=14255, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None)
['/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json']
[2024-05-31 14:00:13,045] [INFO] [comm.py:637:init_distributed] cdb=None
This model is gemma
This model is gemma
/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.54it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]
trainable params: 1,843,200 || all params: 2,508,015,616 || trainable%: 0.073492365368111
trainable params: 1,843,200 || all params: 2,508,015,616 || trainable%: 0.073492365368111
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1685 examples [00:00, 186352.07 examples/s]
Map (num_proc=10):   0%|          | 0/1685 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/1685 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 8/1685 [00:01<06:53,  4.06 examples/s]Map (num_proc=10):   3%|▎         | 58/1685 [00:02<00:43, 37.26 examples/s]Map (num_proc=10):   5%|▌         | 91/1685 [00:02<00:28, 55.73 examples/s]Map (num_proc=10):   7%|▋         | 125/1685 [00:02<00:23, 67.48 examples/s]Map (num_proc=10):  10%|█         | 169/1685 [00:03<00:27, 54.72 examples/s]Map (num_proc=10):  11%|█▏        | 191/1685 [00:04<00:30, 49.79 examples/s]Map (num_proc=10):  13%|█▎        | 213/1685 [00:04<00:24, 59.56 examples/s]Map (num_proc=10):  14%|█▍        | 237/1685 [00:04<00:20, 71.68 examples/s]Map (num_proc=10):  15%|█▌        | 259/1685 [00:04<00:17, 83.32 examples/s]Map (num_proc=10):  16%|█▋        | 275/1685 [00:04<00:16, 88.08 examples/s]Map (num_proc=10):  17%|█▋        | 294/1685 [00:05<00:14, 97.40 examples/s]Map (num_proc=10):  19%|█▉        | 319/1685 [00:05<00:12, 113.59 examples/s]Map (num_proc=10):  20%|██        | 338/1685 [00:06<00:29, 44.98 examples/s] Map (num_proc=10):  21%|██        | 349/1685 [00:06<00:35, 37.17 examples/s]Map (num_proc=10):   1%|          | 9/1685 [00:06<19:53,  1.40 examples/s]Map (num_proc=10):  22%|██▏       | 376/1685 [00:06<00:24, 53.83 examples/s]Map (num_proc=10):   2%|▏         | 35/1685 [00:06<03:57,  6.93 examples/s]Map (num_proc=10):  24%|██▎       | 399/1685 [00:07<00:18, 67.87 examples/s]Map (num_proc=10):   4%|▎         | 61/1685 [00:06<01:54, 14.20 examples/s]Map (num_proc=10):  25%|██▌       | 422/1685 [00:07<00:15, 82.83 examples/s]Map (num_proc=10):   5%|▍         | 81/1685 [00:06<01:14, 21.42 examples/s]Map (num_proc=10):  26%|██▋       | 443/1685 [00:07<00:12, 96.28 examples/s]Map (num_proc=10):   6%|▋         | 106/1685 [00:07<00:47, 32.96 examples/s]Map (num_proc=10):  28%|██▊       | 468/1685 [00:07<00:10, 113.49 examples/s]Map (num_proc=10):   8%|▊         | 128/1685 [00:07<00:34, 44.66 examples/s]Map (num_proc=10):  29%|██▉       | 492/1685 [00:07<00:09, 126.07 examples/s]Map (num_proc=10):   9%|▉         | 153/1685 [00:07<00:25, 60.59 examples/s]Map (num_proc=10):  10%|█         | 170/1685 [00:07<00:22, 67.77 examples/s]Map (num_proc=10):  30%|███       | 513/1685 [00:07<00:11, 100.76 examples/s]Map (num_proc=10):  12%|█▏        | 197/1685 [00:07<00:16, 90.59 examples/s]Map (num_proc=10):  31%|███▏      | 530/1685 [00:08<00:10, 111.25 examples/s]Map (num_proc=10):  13%|█▎        | 224/1685 [00:07<00:13, 109.54 examples/s]Map (num_proc=10):  33%|███▎      | 555/1685 [00:08<00:08, 127.55 examples/s]Map (num_proc=10):  15%|█▍        | 250/1685 [00:07<00:11, 125.39 examples/s]Map (num_proc=10):  34%|███▍      | 579/1685 [00:08<00:08, 137.80 examples/s]Map (num_proc=10):  16%|█▌        | 269/1685 [00:08<00:11, 126.78 examples/s]Map (num_proc=10):  35%|███▌      | 598/1685 [00:08<00:07, 140.61 examples/s]Map (num_proc=10):  18%|█▊        | 296/1685 [00:08<00:09, 148.99 examples/s]Map (num_proc=10):  38%|███▊      | 642/1685 [00:08<00:05, 187.78 examples/s]Map (num_proc=10):  20%|██        | 343/1685 [00:08<00:06, 195.12 examples/s]Map (num_proc=10):  41%|████      | 691/1685 [00:08<00:04, 232.26 examples/s]Map (num_proc=10):  24%|██▍       | 402/1685 [00:08<00:05, 252.95 examples/s]Map (num_proc=10):  44%|████▍     | 743/1685 [00:08<00:03, 269.77 examples/s]Map (num_proc=10):  26%|██▌       | 437/1685 [00:08<00:05, 223.48 examples/s]Map (num_proc=10):  46%|████▌     | 776/1685 [00:09<00:03, 243.94 examples/s]Map (num_proc=10):  28%|██▊       | 477/1685 [00:08<00:05, 211.66 examples/s]Map (num_proc=10):  48%|████▊     | 811/1685 [00:09<00:04, 217.73 examples/s]Map (num_proc=10):  30%|███       | 510/1685 [00:09<00:05, 213.18 examples/s]Map (num_proc=10):  51%|█████     | 853/1685 [00:09<00:03, 235.67 examples/s]Map (num_proc=10):  32%|███▏      | 535/1685 [00:09<00:05, 201.22 examples/s]Map (num_proc=10):  53%|█████▎    | 889/1685 [00:09<00:03, 237.90 examples/s]Map (num_proc=10):  33%|███▎      | 563/1685 [00:09<00:05, 207.22 examples/s]Map (num_proc=10):  55%|█████▍    | 921/1685 [00:09<00:03, 245.03 examples/s]Map (num_proc=10):  37%|███▋      | 624/1685 [00:09<00:03, 287.72 examples/s]Map (num_proc=10):  57%|█████▋    | 956/1685 [00:09<00:02, 260.52 examples/s]Map (num_proc=10):  59%|█████▊    | 987/1685 [00:09<00:02, 265.83 examples/s]Map (num_proc=10):  40%|███▉      | 666/1685 [00:09<00:03, 296.36 examples/s]Map (num_proc=10):  61%|██████▏   | 1034/1685 [00:10<00:02, 301.89 examples/s]Map (num_proc=10):  42%|████▏     | 713/1685 [00:09<00:03, 319.59 examples/s]Map (num_proc=10):  64%|██████▍   | 1078/1685 [00:10<00:01, 336.66 examples/s]Map (num_proc=10):  47%|████▋     | 790/1685 [00:09<00:02, 428.59 examples/s]Map (num_proc=10):  67%|██████▋   | 1137/1685 [00:10<00:01, 375.74 examples/s]Map (num_proc=10):  50%|█████     | 850/1685 [00:09<00:02, 417.07 examples/s]Map (num_proc=10):  70%|███████   | 1185/1685 [00:10<00:01, 372.43 examples/s]Map (num_proc=10):  54%|█████▍    | 907/1685 [00:10<00:01, 420.29 examples/s]Map (num_proc=10):  74%|███████▍  | 1245/1685 [00:10<00:01, 413.19 examples/s]Map (num_proc=10):  56%|█████▋    | 952/1685 [00:10<00:02, 360.20 examples/s]Map (num_proc=10):  77%|███████▋  | 1303/1685 [00:10<00:00, 433.98 examples/s]Map (num_proc=10):  59%|█████▉    | 992/1685 [00:10<00:01, 361.03 examples/s]Map (num_proc=10):  81%|████████  | 1360/1685 [00:10<00:00, 425.64 examples/s]Map (num_proc=10):  62%|██████▏   | 1040/1685 [00:10<00:01, 378.66 examples/s]Map (num_proc=10):  83%|████████▎ | 1404/1685 [00:10<00:00, 420.74 examples/s]Map (num_proc=10):  64%|██████▍   | 1086/1685 [00:10<00:01, 391.09 examples/s]Map (num_proc=10):  87%|████████▋ | 1463/1685 [00:11<00:00, 450.68 examples/s]Map (num_proc=10):  68%|██████▊   | 1144/1685 [00:10<00:01, 426.11 examples/s]Map (num_proc=10):  90%|████████▉ | 1515/1685 [00:11<00:00, 432.19 examples/s]Map (num_proc=10):  73%|███████▎  | 1228/1685 [00:10<00:00, 513.39 examples/s]Map (num_proc=10):  94%|█████████▍| 1583/1685 [00:11<00:00, 480.91 examples/s]Map (num_proc=10):  78%|███████▊  | 1312/1685 [00:10<00:00, 565.76 examples/s]Map (num_proc=10):  82%|████████▏ | 1375/1685 [00:10<00:00, 580.12 examples/s]Map (num_proc=10):  98%|█████████▊| 1650/1685 [00:11<00:00, 379.16 examples/s]Map (num_proc=10):  87%|████████▋ | 1458/1685 [00:11<00:00, 637.28 examples/s]Map (num_proc=10):  92%|█████████▏| 1553/1685 [00:11<00:00, 641.17 examples/s]Map (num_proc=10):  97%|█████████▋| 1632/1685 [00:11<00:00, 670.00 examples/s]Map (num_proc=10): 100%|██████████| 1685/1685 [00:12<00:00, 135.90 examples/s]
Map (num_proc=10): 100%|██████████| 1685/1685 [00:12<00:00, 137.83 examples/s]
Filter (num_proc=10):   0%|          | 0/1685 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/1685 [00:00<?, ? examples/s][2024-05-31 14:00:38,578] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1412641
[2024-05-31 14:00:38,578] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1412642
[2024-05-31 14:00:41,765] [ERROR] [launch.py:325:sigkill_handler] ['/home/chanho/anaconda3/envs/COMEDY/bin/python', '-u', 'training/EPISODE/main.py', '--local_rank=1', '--model_name', 'gemma', '--model_name_or_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path', '--train_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--valid_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--data_output_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', '--max_seq_len', '2048', '--learning_rate', '1e-5', '--weight_decay', '0.1', '--num_train_epochs', '50', '--num_train_samples', '14255', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '1000', '--seed', '42', '--zero_stage', '2', '--save_interval', '1000', '--eval_interval', '100', '--output_dir', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04'] exits with return code = -9
