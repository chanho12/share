[2024-05-31 14:00:06,854] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-05-31 14:00:07,433] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-31 14:00:07,433] [INFO] [runner.py:568:main] cmd = /home/chanho/anaconda3/envs/COMEDY/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=12340 --enable_each_rank_log=None training/EPISODE/main.py --model_name gemma --model_name_or_path /home/chanho/Model/SHARE/Refactorizing/result/output_path --train_data_path /home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json --valid_data_path /home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --data_output_path /home/chanho/Model/SHARE/Refactorizing/result/output_path/data --max_seq_len 2048 --learning_rate 1e-5 --weight_decay 0.1 --num_train_epochs 50 --num_train_samples 14255 /home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json --gradient_accumulation_steps 1 --lr_scheduler_type cosine --num_warmup_steps 1000 --seed 42 --zero_stage 2 --save_interval 1000 --eval_interval 100 --output_dir /home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04
[2024-05-31 14:00:08,915] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-05-31 14:00:09,574] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-05-31 14:00:09,574] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-05-31 14:00:09,574] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-05-31 14:00:09,574] [INFO] [launch.py:164:main] dist_world_size=2
[2024-05-31 14:00:09,574] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-05-31 14:00:09,574] [INFO] [launch.py:256:main] process 1412641 spawned with command: ['/home/chanho/anaconda3/envs/COMEDY/bin/python', '-u', 'training/EPISODE/main.py', '--local_rank=0', '--model_name', 'gemma', '--model_name_or_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path', '--train_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--valid_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--data_output_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', '--max_seq_len', '2048', '--learning_rate', '1e-5', '--weight_decay', '0.1', '--num_train_epochs', '50', '--num_train_samples', '14255', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '1000', '--seed', '42', '--zero_stage', '2', '--save_interval', '1000', '--eval_interval', '100', '--output_dir', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04']
[2024-05-31 14:00:09,575] [INFO] [launch.py:256:main] process 1412642 spawned with command: ['/home/chanho/anaconda3/envs/COMEDY/bin/python', '-u', 'training/EPISODE/main.py', '--local_rank=1', '--model_name', 'gemma', '--model_name_or_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path', '--train_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--valid_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--data_output_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', '--max_seq_len', '2048', '--learning_rate', '1e-5', '--weight_decay', '0.1', '--num_train_epochs', '50', '--num_train_samples', '14255', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '1000', '--seed', '42', '--zero_stage', '2', '--save_interval', '1000', '--eval_interval', '100', '--output_dir', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04']
[2024-05-31 14:00:12,247] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-31 14:00:12,283] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Namespace(data_path=['Dahoas/rm-static'], data_split='6,2,2', data_output_path='/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', model_name_or_path='/home/chanho/Model/SHARE/Refactorizing/result/output_path', model_name='gemma', per_device_train_batch_size=1, per_device_eval_batch_size=1, max_seq_len=2048, learning_rate=1e-05, weight_decay=0.1, num_train_epochs=50, gradient_accumulation_steps=1, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=1000, output_dir='/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04', seed=42, local_rank=0, gradient_checkpointing=False, offload=False, zero_stage=2, lora_dim=0, lora_module_name='decoder.layers.', only_optimize_lora=False, save_interval=1000, log_interval=10, eval_interval=100, train_data_path='/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', valid_data_path='/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', num_train_samples=14255, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None)
['/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json']
[2024-05-31 14:00:12,690] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-31 14:00:12,690] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Namespace(data_path=['Dahoas/rm-static'], data_split='6,2,2', data_output_path='/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', model_name_or_path='/home/chanho/Model/SHARE/Refactorizing/result/output_path', model_name='gemma', per_device_train_batch_size=1, per_device_eval_batch_size=1, max_seq_len=2048, learning_rate=1e-05, weight_decay=0.1, num_train_epochs=50, gradient_accumulation_steps=1, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=1000, output_dir='/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04', seed=42, local_rank=1, gradient_checkpointing=False, offload=False, zero_stage=2, lora_dim=0, lora_module_name='decoder.layers.', only_optimize_lora=False, save_interval=1000, log_interval=10, eval_interval=100, train_data_path='/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', valid_data_path='/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', num_train_samples=14255, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None)
['/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json']
[2024-05-31 14:00:13,045] [INFO] [comm.py:637:init_distributed] cdb=None
This model is gemma
This model is gemma
/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/chanho/anaconda3/envs/COMEDY/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.54it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.33it/s]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.42it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.21it/s]
trainable params: 1,843,200 || all params: 2,508,015,616 || trainable%: 0.073492365368111
trainable params: 1,843,200 || all params: 2,508,015,616 || trainable%: 0.073492365368111
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1685 examples [00:00, 186352.07 examples/s]
Map (num_proc=10):   0%|          | 0/1685 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/1685 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 8/1685 [00:01<06:53,  4.06 examples/s]Map (num_proc=10):   3%|â–Ž         | 58/1685 [00:02<00:43, 37.26 examples/s]Map (num_proc=10):   5%|â–Œ         | 91/1685 [00:02<00:28, 55.73 examples/s]Map (num_proc=10):   7%|â–‹         | 125/1685 [00:02<00:23, 67.48 examples/s]Map (num_proc=10):  10%|â–ˆ         | 169/1685 [00:03<00:27, 54.72 examples/s]Map (num_proc=10):  11%|â–ˆâ–        | 191/1685 [00:04<00:30, 49.79 examples/s]Map (num_proc=10):  13%|â–ˆâ–Ž        | 213/1685 [00:04<00:24, 59.56 examples/s]Map (num_proc=10):  14%|â–ˆâ–        | 237/1685 [00:04<00:20, 71.68 examples/s]Map (num_proc=10):  15%|â–ˆâ–Œ        | 259/1685 [00:04<00:17, 83.32 examples/s]Map (num_proc=10):  16%|â–ˆâ–‹        | 275/1685 [00:04<00:16, 88.08 examples/s]Map (num_proc=10):  17%|â–ˆâ–‹        | 294/1685 [00:05<00:14, 97.40 examples/s]Map (num_proc=10):  19%|â–ˆâ–‰        | 319/1685 [00:05<00:12, 113.59 examples/s]Map (num_proc=10):  20%|â–ˆâ–ˆ        | 338/1685 [00:06<00:29, 44.98 examples/s] Map (num_proc=10):  21%|â–ˆâ–ˆ        | 349/1685 [00:06<00:35, 37.17 examples/s]Map (num_proc=10):   1%|          | 9/1685 [00:06<19:53,  1.40 examples/s]Map (num_proc=10):  22%|â–ˆâ–ˆâ–       | 376/1685 [00:06<00:24, 53.83 examples/s]Map (num_proc=10):   2%|â–         | 35/1685 [00:06<03:57,  6.93 examples/s]Map (num_proc=10):  24%|â–ˆâ–ˆâ–Ž       | 399/1685 [00:07<00:18, 67.87 examples/s]Map (num_proc=10):   4%|â–Ž         | 61/1685 [00:06<01:54, 14.20 examples/s]Map (num_proc=10):  25%|â–ˆâ–ˆâ–Œ       | 422/1685 [00:07<00:15, 82.83 examples/s]Map (num_proc=10):   5%|â–         | 81/1685 [00:06<01:14, 21.42 examples/s]Map (num_proc=10):  26%|â–ˆâ–ˆâ–‹       | 443/1685 [00:07<00:12, 96.28 examples/s]Map (num_proc=10):   6%|â–‹         | 106/1685 [00:07<00:47, 32.96 examples/s]Map (num_proc=10):  28%|â–ˆâ–ˆâ–Š       | 468/1685 [00:07<00:10, 113.49 examples/s]Map (num_proc=10):   8%|â–Š         | 128/1685 [00:07<00:34, 44.66 examples/s]Map (num_proc=10):  29%|â–ˆâ–ˆâ–‰       | 492/1685 [00:07<00:09, 126.07 examples/s]Map (num_proc=10):   9%|â–‰         | 153/1685 [00:07<00:25, 60.59 examples/s]Map (num_proc=10):  10%|â–ˆ         | 170/1685 [00:07<00:22, 67.77 examples/s]Map (num_proc=10):  30%|â–ˆâ–ˆâ–ˆ       | 513/1685 [00:07<00:11, 100.76 examples/s]Map (num_proc=10):  12%|â–ˆâ–        | 197/1685 [00:07<00:16, 90.59 examples/s]Map (num_proc=10):  31%|â–ˆâ–ˆâ–ˆâ–      | 530/1685 [00:08<00:10, 111.25 examples/s]Map (num_proc=10):  13%|â–ˆâ–Ž        | 224/1685 [00:07<00:13, 109.54 examples/s]Map (num_proc=10):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 555/1685 [00:08<00:08, 127.55 examples/s]Map (num_proc=10):  15%|â–ˆâ–        | 250/1685 [00:07<00:11, 125.39 examples/s]Map (num_proc=10):  34%|â–ˆâ–ˆâ–ˆâ–      | 579/1685 [00:08<00:08, 137.80 examples/s]Map (num_proc=10):  16%|â–ˆâ–Œ        | 269/1685 [00:08<00:11, 126.78 examples/s]Map (num_proc=10):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 598/1685 [00:08<00:07, 140.61 examples/s]Map (num_proc=10):  18%|â–ˆâ–Š        | 296/1685 [00:08<00:09, 148.99 examples/s]Map (num_proc=10):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 642/1685 [00:08<00:05, 187.78 examples/s]Map (num_proc=10):  20%|â–ˆâ–ˆ        | 343/1685 [00:08<00:06, 195.12 examples/s]Map (num_proc=10):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 691/1685 [00:08<00:04, 232.26 examples/s]Map (num_proc=10):  24%|â–ˆâ–ˆâ–       | 402/1685 [00:08<00:05, 252.95 examples/s]Map (num_proc=10):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 743/1685 [00:08<00:03, 269.77 examples/s]Map (num_proc=10):  26%|â–ˆâ–ˆâ–Œ       | 437/1685 [00:08<00:05, 223.48 examples/s]Map (num_proc=10):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 776/1685 [00:09<00:03, 243.94 examples/s]Map (num_proc=10):  28%|â–ˆâ–ˆâ–Š       | 477/1685 [00:08<00:05, 211.66 examples/s]Map (num_proc=10):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 811/1685 [00:09<00:04, 217.73 examples/s]Map (num_proc=10):  30%|â–ˆâ–ˆâ–ˆ       | 510/1685 [00:09<00:05, 213.18 examples/s]Map (num_proc=10):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 853/1685 [00:09<00:03, 235.67 examples/s]Map (num_proc=10):  32%|â–ˆâ–ˆâ–ˆâ–      | 535/1685 [00:09<00:05, 201.22 examples/s]Map (num_proc=10):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 889/1685 [00:09<00:03, 237.90 examples/s]Map (num_proc=10):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 563/1685 [00:09<00:05, 207.22 examples/s]Map (num_proc=10):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 921/1685 [00:09<00:03, 245.03 examples/s]Map (num_proc=10):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 624/1685 [00:09<00:03, 287.72 examples/s]Map (num_proc=10):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 956/1685 [00:09<00:02, 260.52 examples/s]Map (num_proc=10):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 987/1685 [00:09<00:02, 265.83 examples/s]Map (num_proc=10):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 666/1685 [00:09<00:03, 296.36 examples/s]Map (num_proc=10):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1034/1685 [00:10<00:02, 301.89 examples/s]Map (num_proc=10):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 713/1685 [00:09<00:03, 319.59 examples/s]Map (num_proc=10):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1078/1685 [00:10<00:01, 336.66 examples/s]Map (num_proc=10):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 790/1685 [00:09<00:02, 428.59 examples/s]Map (num_proc=10):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1137/1685 [00:10<00:01, 375.74 examples/s]Map (num_proc=10):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 850/1685 [00:09<00:02, 417.07 examples/s]Map (num_proc=10):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1185/1685 [00:10<00:01, 372.43 examples/s]Map (num_proc=10):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 907/1685 [00:10<00:01, 420.29 examples/s]Map (num_proc=10):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1245/1685 [00:10<00:01, 413.19 examples/s]Map (num_proc=10):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 952/1685 [00:10<00:02, 360.20 examples/s]Map (num_proc=10):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1303/1685 [00:10<00:00, 433.98 examples/s]Map (num_proc=10):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 992/1685 [00:10<00:01, 361.03 examples/s]Map (num_proc=10):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1360/1685 [00:10<00:00, 425.64 examples/s]Map (num_proc=10):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1040/1685 [00:10<00:01, 378.66 examples/s]Map (num_proc=10):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1404/1685 [00:10<00:00, 420.74 examples/s]Map (num_proc=10):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1086/1685 [00:10<00:01, 391.09 examples/s]Map (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1463/1685 [00:11<00:00, 450.68 examples/s]Map (num_proc=10):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1144/1685 [00:10<00:01, 426.11 examples/s]Map (num_proc=10):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1515/1685 [00:11<00:00, 432.19 examples/s]Map (num_proc=10):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1228/1685 [00:10<00:00, 513.39 examples/s]Map (num_proc=10):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1583/1685 [00:11<00:00, 480.91 examples/s]Map (num_proc=10):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1312/1685 [00:10<00:00, 565.76 examples/s]Map (num_proc=10):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1375/1685 [00:10<00:00, 580.12 examples/s]Map (num_proc=10):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1650/1685 [00:11<00:00, 379.16 examples/s]Map (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1458/1685 [00:11<00:00, 637.28 examples/s]Map (num_proc=10):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1553/1685 [00:11<00:00, 641.17 examples/s]Map (num_proc=10):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1632/1685 [00:11<00:00, 670.00 examples/s]Map (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1685/1685 [00:12<00:00, 135.90 examples/s]
Map (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1685/1685 [00:12<00:00, 137.83 examples/s]
Filter (num_proc=10):   0%|          | 0/1685 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/1685 [00:00<?, ? examples/s][2024-05-31 14:00:38,578] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1412641
[2024-05-31 14:00:38,578] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1412642
[2024-05-31 14:00:41,765] [ERROR] [launch.py:325:sigkill_handler] ['/home/chanho/anaconda3/envs/COMEDY/bin/python', '-u', 'training/EPISODE/main.py', '--local_rank=1', '--model_name', 'gemma', '--model_name_or_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path', '--train_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--valid_data_path', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/valid_without_tag.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--data_output_path', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/data', '--max_seq_len', '2048', '--learning_rate', '1e-5', '--weight_decay', '0.1', '--num_train_epochs', '50', '--num_train_samples', '14255', '/home/chanho/Model/SHARE/Refactorizing/result/dataset/train_without_tag.json', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '1000', '--seed', '42', '--zero_stage', '2', '--save_interval', '1000', '--eval_interval', '100', '--output_dir', '/home/chanho/Model/SHARE/Refactorizing/result/output_path/2024-05-31-14.00.04'] exits with return code = -9
